---
date: 2024-08-21
category:
  - topic
---
# GPT Overview

## GPT Models
- **GPT-1 (2018)**: Foundation of pre-trained transformers with 117 million parameters.
- **GPT-2 (2019)**: Scaled up to 1.5 billion parameters.
- **GPT-3 (2020)**: 175 billion parameters, capable of generating and classifying text with remarkable ease.
- **GPT-4 (2023)**: Latest version with enhanced capabilities.

## Datasets Used
- Common Crawl
- WebText2
- Books1 and Books2
- Wikipedia

[[llm#Applications of LLMs]]
[[Prompt-Engineering]]]
[[Tokenization]]
